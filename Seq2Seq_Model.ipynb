{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Old_json_Seq2Seq",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO5jM0NBEEBb6+TQsTpEGdE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/savitha91/SpeechToImage/blob/master/Seq2Seq_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAUCqYUXGCdL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import torch.nn as nn"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2wNJQ7qKCw_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://github.com/AladdinPerzon/Machine-Learning-Collection/blob/master/ML/Pytorch/more_advanced/Seq2Seq_attention/seq2seq_attention.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jRkHHN9CbB8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "affd469a-a220-4875-8ce2-38b476e80ee0"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFAqhLieHd1S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "JSON_PATH = '/content/data_old.json'\n"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4rkH39rGKQA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def addPadding(X):\n",
        "    lenLi = [len(i) for i in X]\n",
        "    maxLen = np.max(lenLi)\n",
        "    zer = np.zeros(10).tolist()\n",
        "    for i in X:\n",
        "        if(len(i) < maxLen):\n",
        "            diff = maxLen - len(i)\n",
        "            for j in range(diff):\n",
        "                i.append(zer)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sOMSZuTbdzY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# add <pad> tag at end\n",
        "def addPaddingY(sentence,maxLen):\n",
        "  #print(sentence)\n",
        "  #print(\"BEFOR.  \" , len(sentence))\n",
        "  if(len(sentence) < maxLen):\n",
        "    diff = maxLen - len(sentence)\n",
        "    for j in range(diff):\n",
        "      sentence = sentence + ' '\n",
        "  #print(\"AFTER.  \" , len(sentence))\n",
        "  return sentence"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lI-C4CrrV_AF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "#Not adding any padding and squeeze\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n",
        "bertModel = DistilBertModel.from_pretrained('distilbert-base-cased')\n"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TANrb4dVrih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Uses BERT Model for Embedding\n",
        "def load_data(data_path):\n",
        "    with open(data_path, \"r\") as fp:\n",
        "        data = json.load(fp)\n",
        "    X = data['MFCCs']\n",
        "    addPadding(X) #101,10 ; 200,10\n",
        "    X = np.array(X)\n",
        "    y = []\n",
        "    y_text = np.array(data[\"utterances\"])\n",
        "    for sentence in y_text:\n",
        "      inputs = tokenizer.encode(sentence, add_special_tokens=True,max_length=20, pad_to_max_length=True,truncation=True)\n",
        "      #ten = torch.tensor(inputs).unsqueeze(0)\n",
        "      #outputs = bertModel(ten) # returns tuple (embeddings , requires_grad = True)\n",
        "      #sq_output =  outputs[0].squeeze(0) #removing the batc size\n",
        "      #arr = sq_output.detach().numpy() # outputs[0] is a tuple , converts this to numpy array\n",
        "      y.append(inputs)\n",
        "\n",
        "    y = np.array(y)\n",
        "    return X,y"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbPBvMmVKl8R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_dataset(data_path, test_size=0.2, validation_size=0.2):\n",
        "    # load dataset\n",
        "    X, y = load_data(data_path)\n",
        "    # create train, validation, test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size,shuffle=True)\n",
        "    X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=validation_size,shuffle=True)\n",
        "    return X_train, y_train, X_validation, y_validation, X_test, y_test"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iU-rpCO2sSl7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.utils.vis_utils import plot_model\n",
        "# configure\n",
        "X_train, y_train, X_validation, y_validation, X_test, y_test = prepare_dataset(JSON_PATH)\n"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDMxuKzpjlUm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "085527a0-e3d4-45f0-f82a-bc21f1a259ae"
      },
      "source": [
        "len(X_train) #708\n",
        "len(X_validation) #178\n",
        "len(X_test) #222\n"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlQz0wOLe6PF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0c6e4d47-59dd-4c9b-d0ad-c785a935b8be"
      },
      "source": [
        "o = tokenizer.decode(y_test[1])\n",
        "o \n",
        "#[CLS] No - no, thank you. [SEP] "
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'[CLS] No, I know! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnId5vKvHAai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, p):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.dropout = nn.Dropout(p)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        #self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers, dropout=p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (seq_length, N) where N is batch size\n",
        "\n",
        "        #embedding = self.dropout(self.embedding(x))\n",
        "        # embedding shape: (seq_length, N, embedding_size)\n",
        "\n",
        "        outputs, (hidden, cell) = self.rnn(x)\n",
        "        #output (seq_len, batch, hidden_size * num_directions)\n",
        "        #h_n (num_layers * num_directions, batch, hidden_size)\n",
        "        #c_n (num_layers * num_directions, batch, hidden_size)\n",
        "        # outputs shape torch.Size([101, 2, 1024])\n",
        "        # hidden shape torch.Size([2, 2, 1024])\n",
        "        # cell shape torch.Size([2, 2, 1024])\n",
        "        return hidden, cell\n"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFDOd1hAHEJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "input_size_encoder = 10\n",
        "hidden_size = 1024  # Needs to be the same for both RNN's\n",
        "num_layers = 2\n",
        "enc_dropout = 0.5\n",
        "\n",
        "encoder_net = Encoder(\n",
        "    input_size_encoder, hidden_size, num_layers, enc_dropout\n",
        ").to(device)"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vv5MnBevHU6w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "2178f47d-5c68-461c-f04a-c16d91aeeeb9"
      },
      "source": [
        "encoder_net"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Encoder(\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (rnn): LSTM(10, 1024, num_layers=2, dropout=0.5)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMgmBqCjIPa5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#As Embedding is done, we need to use Embedding size. If Embedding not done we use vocab size, for which Embeddings are created by the Embedding layer\n",
        "input_size_decoder = 768\n",
        "output_size = tokenizer.vocab_size #28996\n",
        "#decoder_embedding_size = 400\n",
        "hidden_size = 1024  # Needs to be the same for both RNN's\n",
        "dec_dropout = 0.5\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self, input_size, hidden_size, output_size, num_layers, p\n",
        "    ):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.dropout = nn.Dropout(p)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        #self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers, dropout=p)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        x = x.unsqueeze(0)\n",
        "        embedX = bertModel(x) # returns tuple (embeddings , requires_grad = True)\n",
        "        x =  embedX[0]\n",
        "        # x shape: (N) where N is for batch size, we want it to be (1, N), seq_length\n",
        "        # is 1 here because we are sending in a single word and not a sentence\n",
        "        #print(\"Before squeeze\",x.shape)\n",
        "        #x = x.unsqueeze(0)\n",
        "        #embedding = self.dropout(self.embedding(x))\n",
        "        #print(\"Decoder embedding done\")\n",
        "        #print(\"decoder embedding shape\", embedding.shape)\n",
        "        # embedding shape: (1, N, embedding_size)\n",
        "\n",
        "        outputs, (hidden, cell) = self.rnn(x, (hidden, cell))\n",
        "        # outputs shape: (1, N, hidden_size)\n",
        "        predictions = self.fc(outputs)\n",
        "        # predictions shape: (1, N, length_target_vocabulary) to send it to\n",
        "        # loss function we want it to be (N, length_target_vocabulary) so we're\n",
        "        # just gonna remove the first dim\n",
        "        predictions = predictions.squeeze(0)\n",
        "\n",
        "        return predictions, hidden, cell"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOvKFjSAQfK4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_net = Decoder(\n",
        "    input_size_decoder,\n",
        "    hidden_size,\n",
        "    output_size,\n",
        "    num_layers,\n",
        "    dec_dropout,\n",
        ").to(device)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYeQ4eiUQzvP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "d191b02b-6b96-4781-9749-e57b0be85974"
      },
      "source": [
        "decoder_net"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Decoder(\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (rnn): LSTM(768, 1024, num_layers=2, dropout=0.5)\n",
              "  (fc): Linear(in_features=1024, out_features=28996, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3mVct7vQ5Zy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
        "        batch_size = source.shape[1] \n",
        "        target_len = target.shape[0]\n",
        "        target_vocab_size = tokenizer.vocab_size\n",
        "\n",
        "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
        "        hidden, cell = self.encoder(source)\n",
        "        # Grab the first input to the Decoder which will be <SOS> token\n",
        "        x = target[0]\n",
        "        # Default LSTM without the flags, return_state = true or return_sequences =True, returns hidden_state of last time-step as output\n",
        "        # Seq-to-Seq model, usually has Encoder with LSTM , return_state = True , returns output (=hidden state(h)) , hidden state(h), cell state(c) from the last time step\n",
        "        # Decoder with LSTM , return_state =True, return_sequences = True, returns output=hidden state(h) from each time step, hidden state(h), cell state(c) from the last time step. output of the last time step = hidden state results\n",
        "        # In pytorch, we dont have these flags, thus, to get the hidden/cell state from each time-step, we take each word, pass it throight decoder LSTM and get the hidden/cell state\n",
        "            # Use previous hidden, cell as context from encoder at start\n",
        "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
        "            # Store next output prediction\n",
        "            outputs[t] = output\n",
        "            # Get the best word the Decoder predicted (index in the vocabulary)\n",
        "            best_guess = output.argmax(1)\n",
        "            # With probability of teacher_force_ratio we take the actual next word\n",
        "            # otherwise we take the word that the Decoder predicted it to be.\n",
        "            # Teacher Forcing is used so that the model gets used to seeing\n",
        "            # similar inputs at training and testing time, if teacher forcing is 1\n",
        "            # then inputs at test time might be completely different than what the\n",
        "            # network is used to. This was a long comment.\n",
        "            #Actual next word already has embedding. Next word predicted by the decoder, doesnt have embeddings\n",
        "            if random.random() < teacher_force_ratio:\n",
        "              x = target[t] #Actual next word\n",
        "            else:\n",
        "              x = best_guess\n",
        "            \n",
        "        return outputs"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WulQrioMRIJm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "load_model = False\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHr08EYuRIZ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "learning_rate = 0.001\n",
        "model = Seq2Seq(encoder_net, decoder_net).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "pad_idx = 0\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3l5sDKtXSJ6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "52db4f3d-fef7-425f-c9f0-5c559748ac50"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmxhPkqeRN9V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tensor_x = torch.tensor(X_train)\n",
        "tensor_y = torch.tensor(y_train)\n",
        "\n",
        "tensor_xval = torch.tensor(X_validation)\n",
        "tensor_yval = torch.tensor(y_validation)\n",
        "\n",
        "tensor_xtest = torch.tensor(X_test)\n",
        "tensor_ytest = torch.tensor(y_test)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awmslcPWV4HX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import random\n",
        "\n",
        "train_set = TensorDataset(tensor_x,tensor_y) # create your datset\n",
        "train_loader = DataLoader(train_set,batch_size=2)\n",
        "\n",
        "val_set = TensorDataset(tensor_xval,tensor_yval) # create your datset\n",
        "val_loader = DataLoader(val_set,batch_size=2)\n",
        "\n",
        "test_set = TensorDataset(tensor_xtest,tensor_ytest) # create your datset\n",
        "test_loader = DataLoader(test_set,batch_size=2)"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-TNzgrqqKCm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    torch.save(state, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint, model, optimizer):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liJyXQCYv1WV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X_sample = X_test[3]\n",
        "# Convert to tensor,\n",
        "\n",
        "def recognizeSpeech(model, X,  device, max_length):\n",
        "  tensor_sample = torch.tensor(X).unsqueeze(1).to(device)\n",
        "  with torch.no_grad():\n",
        "        hidden, cell = model.encoder(tensor_sample.float())\n",
        "  outputs = [101]\n",
        "  for _ in range(max_length):\n",
        "    previous_word = torch.LongTensor([outputs[-1]]).to(device)\n",
        "    with torch.no_grad():\n",
        "      output, hidden, cell = model.decoder(previous_word, hidden, cell)\n",
        "      best_guess = output.argmax(1).item()\n",
        "\n",
        "    outputs.append(best_guess)\n",
        "\n",
        "    # Model predicts it's the end of the sentence\n",
        "    if output.argmax(1).item() == 102:\n",
        "      break\n",
        "\n",
        "\n",
        "  translated_sentence = [tokenizer.decode(idx) for idx in outputs]\n",
        "  # remove start token\n",
        "  return translated_sentence[1:]"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlXUvmhuoWA8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 769
        },
        "outputId": "b4d7b103-7bc7-403e-e6f6-a685e8eb557b"
      },
      "source": [
        "#Shape of y, requires grad for decoder ???\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
        "\n",
        "    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n",
        "    #save_checkpoint(checkpoint)\n",
        "    \n",
        "    model.eval()\n",
        "    recognizedText = recognizeSpeech(\n",
        "        model, X_test[0],  device, max_length=20\n",
        "    )\n",
        "\n",
        "    print(f\"Recognized Text: \\n {recognizedText}\")   \n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, (X_train,y_train) in enumerate(train_loader):\n",
        "        # Get input and targets and get to cuda\n",
        "        inp_data = X_train.to(device)\n",
        "        target = y_train.to(device)\n",
        "        # Forward prop\n",
        "        inp_data = torch.transpose(inp_data, 0, 1)\n",
        "        target = torch.transpose(target, 0, 1) #[20,2] : 2 examples, each example having 20 words. Col0 has 20 words indexes\n",
        "        output = model(inp_data.float(), target) #[20, 2, 28996] : 2 examples, each example having 20 words\n",
        "        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n",
        "        # doesn't take input in that form. For example if we have MNIST we want to have\n",
        "        # output to be: (N, 10) and targets just (N). Here we can view it in a similar\n",
        "        # way that we have output_words * batch_size that we want to send in into\n",
        "        # our cost function, so we need to do some reshapin. While we're at it\n",
        "        # Let's also remove the start token while we're at it\n",
        "        output = output[1:].reshape(-1, output.shape[2])\n",
        "        target = target[1:].reshape(-1)\n",
        "        loss = criterion(output, target)\n",
        "        # Back prop\n",
        "        loss.backward()\n",
        "        # Clip to avoid exploding gradient issues, makes sure grads are\n",
        "        # within a healthy range\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "        # Gradient descent step\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        # Plot to tensorboard\n",
        "       "
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch 0 / 10]\n",
            "outputs shape torch.Size([101, 1, 1024])\n",
            "hidden shape torch.Size([2, 1, 1024])\n",
            "cell shape torch.Size([2, 1, 1024])\n",
            "Recognized Text: \n",
            " ['P r i n c i p l e s', 'C o n t e s t', 'C o n t e s t', 'a u c t i o n', 'a u c t i o n', 'a u c t i o n', 'L a z a r u s', 's t a m p', 's n e a k i n g', 's n e a k i n g', 'P a r a c h u t e', 'c o c k t a i l', 'R e c e n t l y', 's n e a k i n g', 'P a r a c h u t e', 'R e c e n t l y', 'R e c e n t l y', 'R e c e n t l y', 's n e a k i n g', 'r i v a l']\n",
            "outputs shape torch.Size([101, 2, 1024])\n",
            "hidden shape torch.Size([2, 2, 1024])\n",
            "cell shape torch.Size([2, 2, 1024])\n",
            "outputs shape torch.Size([101, 2, 1024])\n",
            "hidden shape torch.Size([2, 2, 1024])\n",
            "cell shape torch.Size([2, 2, 1024])\n",
            "outputs shape torch.Size([101, 2, 1024])\n",
            "hidden shape torch.Size([2, 2, 1024])\n",
            "cell shape torch.Size([2, 2, 1024])\n",
            "outputs shape torch.Size([101, 2, 1024])\n",
            "hidden shape torch.Size([2, 2, 1024])\n",
            "cell shape torch.Size([2, 2, 1024])\n",
            "outputs shape torch.Size([101, 2, 1024])\n",
            "hidden shape torch.Size([2, 2, 1024])\n",
            "cell shape torch.Size([2, 2, 1024])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-90-776bf6820335>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Back prop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;31m# Clip to avoid exploding gradient issues, makes sure grads are\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# within a healthy range\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jysv0n_d4yRB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}