{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeMo_Audio.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1_ebH0d1_LSji647PESIHhUxS76SCglvF",
      "authorship_tag": "ABX9TyMZWQkXgRcj/lvRmXDplNV/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/savitha91/SpeechToImage/blob/master/NeMo_Audio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1OT-JRHHan6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tarfile\n",
        "\n",
        "zipPath ='/content/drive/My Drive/MadeWithML_Project/datasets/MELD/sample/dev.tar.gz'\n",
        "\n",
        "if zipPath.endswith(\"tar.gz\"):\n",
        "    tar = tarfile.open(zipPath, \"r:gz\")\n",
        "    tar.extractall('/content/VideoFile')\n",
        "    tar.close()\n",
        "elif zipPath.endswith(\"tar\"):\n",
        "    tar = tarfile.open(zipPath, \"r:\")\n",
        "    tar.extractall('/content/VideoFile')\n",
        "    tar.close()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b9sr314IbA6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "45af41ab-d7b1-4c0b-e835-04c07c62f001"
      },
      "source": [
        "import os\n",
        "import pathlib\n",
        "extractedFilesPath = '/content/VideoFile/dev_splits_complete'\n",
        "count = 0\n",
        "for path in pathlib.Path(extractedFilesPath).iterdir():\n",
        "    if path.is_file():\n",
        "        count += 1\n",
        "\n",
        "print(\"Number of video files available\", count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of video files available 1112\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swPLSxJNLjmn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "7ede0162-3886-45a1-c996-63be3b09f29f"
      },
      "source": [
        "! pip install ffmpeg\n",
        "! pip install librosa\n",
        "#!pip install unidecode"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ffmpeg\n",
            "  Downloading https://files.pythonhosted.org/packages/f0/cc/3b7408b8ecf7c1d20ad480c3eaed7619857bf1054b690226e906fdf14258/ffmpeg-1.4.tar.gz\n",
            "Building wheels for collected packages: ffmpeg\n",
            "  Building wheel for ffmpeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpeg: filename=ffmpeg-1.4-cp36-none-any.whl size=6084 sha256=1b42e5fad4930182fd005fa72520ae6b3aab2968ae1cdd7d7e26f1e20d36b351\n",
            "  Stored in directory: /root/.cache/pip/wheels/b6/68/c3/a05a35f647ba871e5572b9bbfc0b95fd1c6637a2219f959e7a\n",
            "Successfully built ffmpeg\n",
            "Installing collected packages: ffmpeg\n",
            "Successfully installed ffmpeg-1.4\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.6/dist-packages (0.6.3)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (2.1.8)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.16.0)\n",
            "Requirement already satisfied: numpy>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.18.5)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: resampy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.2.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.4.1)\n",
            "Requirement already satisfied: numba>=0.38.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.48.0)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.22.2.post1)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.15.0)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.38.0->librosa) (0.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba>=0.38.0->librosa) (49.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2kaVAOcX92j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "66c59a8a-42b3-44d5-968d-b71768ab5c86"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "import codecs\n",
        "#import unidecode\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "extractedFilesPath = '/content/VideoFile/dev_splits_complete'\n",
        "csvPath = '/content/drive/My Drive/MadeWithML_Project/datasets/MELD/sample/dev_sent_emo.csv'\n",
        "manifestJsonPath = '/content/VideoFile/manifestJson.json'\n",
        "\n",
        "def normalize_str(txt) -> str:\n",
        "    # TODO: REPLACE WITH YOUR OWN NORMALIZATION LOGIC HERE!!!!   \n",
        "    valid_chars = (\" \", \"'\", \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\")\n",
        "    new_txt = unidecode.unidecode(txt.lower().strip())\n",
        "    res_arr = []\n",
        "    for c in new_txt:\n",
        "        if c in valid_chars:\n",
        "            res_arr.append(c)\n",
        "        else:\n",
        "            res_arr.append(' ')\n",
        "    res = ''.join(res_arr).strip()    \n",
        "    return ' '.join(res.split())\n",
        "\n",
        "def csv_to_manifest(csv_file, manifest_file):\n",
        "  manifests = []\n",
        "  df = pd.read_csv(csv_file, encoding='utf8')\n",
        "  for index, row in df.iterrows():\n",
        "    try:\n",
        "      entry = {} #Dictionary\n",
        "      wav_folder = '/content/VideoFile/wav'\n",
        "      os.system('mkdir -p {}'.format(wav_folder))\n",
        "      mp4_file = \"dia\"+str(row['Dialogue_ID'])+\"_\"+\"utt\"+str(row['Utterance_ID'])+\".mp4\"\n",
        "      actual_filename = mp4_file[:-4]\n",
        "      wav_file_Path = \"{0}/{1}\".format(wav_folder,actual_filename) + \".wav\"\n",
        "      os.system('ffmpeg -i {}/{} -acodec pcm_s16le -ac 1 -ar 16000 {}/{}.wav'.format(extractedFilesPath,mp4_file,wav_folder, actual_filename))      \n",
        "      samples,sample_rate= librosa.load(wav_file_Path, sr=16000)\n",
        "      #duration=librosa.core.get_duration(filename='/content/VideoFile/wav/dia0_utt0.wav')\n",
        "      audio_duration=np.round(samples.shape[0]/sample_rate , 3)\n",
        "      entry['audio_filepath'] = wav_file_Path\n",
        "      entry['duration'] = float(audio_duration)\n",
        "      entry['text'] = row['Utterance']\n",
        "      manifests.append(entry)\n",
        "    except:\n",
        "      print(\"SOMETHING WENT WRONG with\", wav_folder)\n",
        "\n",
        "  with codecs.open(manifest_file, 'w', encoding='utf-8') as fout:\n",
        "    for m in manifests:\n",
        "      fout.write(json.dumps(m, ensure_ascii=False) + '\\n')\n",
        "  print('Done!')\n",
        "\n",
        "\n",
        "def main():\n",
        "  csv_to_manifest(csvPath, manifestJsonPath)\n",
        "  \n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SOMETHING WENT WRONG with /content/VideoFile/wav\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61GeD0uumP1W",
        "colab_type": "text"
      },
      "source": [
        "import chardet\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "with open(csvPath, 'rb') as f:\n",
        "    result = chardet.detect(f.read())  \n",
        "\n",
        "Check for encoding type when the csv was initially created"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Z5YNQ8xmbnA",
        "colab_type": "text"
      },
      "source": [
        "----- Convert utf encoded string to ascii-----\n",
        "\n",
        "\n",
        "b'Oh my God, he\\x92s lost it. He\\x92s totally lost it.'.decode('windows-1252')\n",
        "\n",
        "------ Result : Oh my God, he’s lost it. He’s totally lost it.------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCyWaO6Qm0uP",
        "colab_type": "text"
      },
      "source": [
        "-----base64 encoding and decoding a string ----\n",
        "\n",
        "import base64\n",
        "Str = \"this is string example....wow!!!\"\n",
        "encoded = base64.b64encode(b'this is string example....wow!!!'). \n",
        "data = base64.b64decode(encoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9gCbLKxxYAE",
        "colab_type": "text"
      },
      "source": [
        "--- zip a file---\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "zf = zipfile.ZipFile(\"wavFiles.zip\", \"w\")\n",
        "for dirname, subdirs, files in os.walk(\"mydirectory\"):\n",
        "    zf.write(dirname)\n",
        "    for filename in files:\n",
        "        zf.write(os.path.join(dirname, filename))\n",
        "zf.close()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUlhM9End8Pv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "d27c0eb6-76f4-48ea-c11f-fd797831de64"
      },
      "source": [
        "!python --version\n",
        "!pip install deepspeech==0.6.0\n",
        "# https://colab.research.google.com/github/scgupta/yearn2learn/blob/master/speech/asr/deepspeech06/mozilla_deepspeech_api_notebook.ipynb#scrollTo=XdzZteC7TZDP\n",
        "!curl -LO https://github.com/mozilla/DeepSpeech/releases/download/v0.6.0/deepspeech-0.6.0-models.tar.gz\n",
        "!tar -xvzf deepspeech-0.6.0-models.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   641  100   641    0     0   2823      0 --:--:-- --:--:-- --:--:--  2823\n",
            "100 1172M  100 1172M    0     0  30.4M      0  0:00:38  0:00:38 --:--:-- 32.0M\n",
            "deepspeech-0.6.0-models/\n",
            "deepspeech-0.6.0-models/lm.binary\n",
            "deepspeech-0.6.0-models/output_graph.pbmm\n",
            "deepspeech-0.6.0-models/output_graph.pb\n",
            "deepspeech-0.6.0-models/trie\n",
            "deepspeech-0.6.0-models/output_graph.tflite\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMd2Nj1yybGn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "9a4d28fd-e11c-4165-897c-957fd903bd31"
      },
      "source": [
        "!deepspeech --model deepspeech-0.6.0-models/output_graph.pb --lm deepspeech-0.7.0-models/lm.binary --trie ./deepspeech-0.6.0-models/trie --audio /content/VideoFile/wav/dia10_utt0.wav"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading model from file deepspeech-0.6.0-models/output_graph.pb\n",
            "TensorFlow: v1.14.0-21-ge77504a\n",
            "DeepSpeech: v0.6.0-0-g6d43e21\n",
            "Warning: reading entire model file into memory. Transform model file into an mmapped graph to reduce heap usage.\n",
            "2020-07-27 06:04:52.732192: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
            "Loaded model in 0.128s.\n",
            "Loading language model from files deepspeech-0.7.0-models/lm.binary ./deepspeech-0.6.0-models/trie\n",
            "[native_client/ctcdecode/scorer.cpp:77] FATAL: \"(access(filename, 4)) == (0)\" check failed. Invalid language model path"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCJdNoV41S0k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import deepspeech\n",
        "model_file_path = '/content/deepspeech-0.6.0-models/output_graph.pbmm'\n",
        "beam_width = 500\n",
        "model = deepspeech.Model(model_file_path, beam_width)\n",
        "lm_file_path = '/content/deepspeech-0.6.0-models/lm.binary'\n",
        "trie_file_path = '/content/deepspeech-0.6.0-models/trie'\n",
        "lm_alpha = 0.75\n",
        "lm_beta = 1.85\n",
        "model.enableDecoderWithLM(lm_file_path, trie_file_path, lm_alpha, lm_beta)\n",
        "import wave\n",
        "filename = '/content/VideoFile/wav/dia0_utt0.wav'\n",
        "w = wave.open(filename, 'r')\n",
        "rate = w.getframerate()\n",
        "frames = w.getnframes()\n",
        "buffer = w.readframes(frames)\n",
        "print(rate)\n",
        "print(model.sampleRate())\n",
        "print(str(type(buffer)))\n",
        "text = model.stt(data16)\n",
        "print(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "du2VeE3jK_Ad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#quartzModelPath = '/content/multidataset_quartznet15x5_2.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4iLcekoQwKb",
        "colab_type": "code",
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "1f545e34-a514-48fa-fe47-765670cc48e1"
      },
      "source": [
        " #@title\n",
        "import os\n",
        "from os.path import exists, join, basename, splitext\n",
        "#from IPython.display import YouTubeVideo\n",
        "\n",
        "#!pip -q install wget youtube-dl wget tensorboardX kaldi-io marshmallow num2words ruamel.yaml soundfile sox torch-stft unidecode\n",
        "!pip install -q nemo-toolkit==0.9.0 nemo-asr==0.9.0 #--no-deps\n",
        "!pip install nemo_toolkit[nlp] #BERT pre-trained LM\n",
        "\n",
        "# apt-get update && apt-get install -y libsndfile1 ffmpeg && pip install Cython\n",
        "# we need also Apex\n",
        "if not exists('apex'):\n",
        "  !git clone -q --depth 1 https://github.com/NVIDIA/apex\n",
        "  !cd apex && pip install -q --no-cache-dir ./"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |████▎                           | 10kB 26.5MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 20kB 6.1MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 30kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 40kB 7.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 51kB 7.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 61kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 71kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 5.9MB/s \n",
            "\u001b[?25h\u001b[?25l\r\u001b[K     |███████▎                        | 10kB 33.7MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 20kB 38.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 30kB 29.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 40kB 20.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 7.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 317kB 12.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.4MB 18.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 13.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 7.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 245kB 51.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 112kB 64.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 552kB 53.6MB/s \n",
            "\u001b[?25h  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for apex (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8ANb8k0Q7cE",
        "colab_type": "code",
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "090ff047-0840-4125-df30-0035940b523c"
      },
      "source": [
        "#@title\n",
        "if not exists('quartznet15x5_multidataset'):\n",
        "  # download the pretrained weights\n",
        "  !wget -nc -q --show-progress -O quartznet15x5.zip https://api.ngc.nvidia.com/v2/models/nvidia/multidataset_quartznet15x5/versions/1/zip\n",
        "  !unzip quartznet15x5.zip && mkdir quartznet15x5_multidataset && mv Jasper* quartznet15x5.yaml quartznet15x5_multidataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "quartznet15x5.zip   100%[===================>]  67.86M  19.1MB/s    in 4.7s    \n",
            "Archive:  quartznet15x5.zip\n",
            "  inflating: JasperDecoderForCTC-STEP-243800.pt  \n",
            "  inflating: JasperEncoder-STEP-243800.pt  \n",
            "  inflating: quartznet15x5.yaml      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPuPTslXVNFV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "dd86ae6f-8a43-4132-b451-b600c6b58663"
      },
      "source": [
        "\n",
        "import json\n",
        "from ruamel.yaml import YAML\n",
        "import nemo\n",
        "import nemo_asr\n",
        "\n",
        "WORK_DIR = \"/content/quartznet15x5_multidataset\"\n",
        "MODEL_YAML = \"/content/quartznet15x5_multidataset/quartznet15x5.yaml\"\n",
        "CHECKPOINT_ENCODER = \"/content/quartznet15x5_multidataset/JasperEncoder-STEP-243800.pt\"\n",
        "CHECKPOINT_DECODER = \"/content/quartznet15x5_multidataset/JasperDecoderForCTC-STEP-243800.pt\"\n",
        "# Set this to True to enable beam search decoder\n",
        "ENABLE_NGRAM = False\n",
        "# This is only necessary if ENABLE_NGRAM = True. Otherwise, set to empty string\n",
        "LM_PATH = \"<PATH_TO_KENLM_BINARY>\"\n",
        "\n",
        "\n",
        "# Instantiate necessary Neural Modules\n",
        "# Note that data layer is missing from here\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Could not import torchaudio. Some features might not work.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "################################################################################\n",
            "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
            "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
            "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
            "################################################################################\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3yBZXy5dVeX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def quartzNetMultiDatasetModel_NoLM(manifest, greedy=True):\n",
        "    from ruamel.yaml import YAML\n",
        "    yaml = YAML(typ=\"safe\")\n",
        "    with open(MODEL_YAML) as f:\n",
        "        jasper_model_definition = yaml.load(f)\n",
        "    labels = jasper_model_definition['labels']\n",
        "\n",
        "    neural_factory = nemo.core.NeuralModuleFactory(\n",
        "    placement=nemo.core.DeviceType.GPU,\n",
        "    log_dir='/content/tb_logs/',\n",
        "    backend=nemo.core.Backend.PyTorch,create_tb_writer=True)\n",
        "\n",
        "    data_preprocessor = nemo_asr.AudioToMelSpectrogramPreprocessor(factory=neural_factory)\n",
        "\n",
        "    jasper_encoder = nemo_asr.JasperEncoder(\n",
        "      jasper=jasper_model_definition['JasperEncoder']['jasper'],\n",
        "      activation=jasper_model_definition['JasperEncoder']['activation'],\n",
        "      feat_in=jasper_model_definition['AudioToMelSpectrogramPreprocessor']['features'])\n",
        "\n",
        "    jasper_encoder.restore_from(CHECKPOINT_ENCODER, local_rank=0)\n",
        "\n",
        "\n",
        "    jasper_decoder = nemo_asr.JasperDecoderForCTC(\n",
        "        feat_in=1024,\n",
        "        num_classes=len(labels))\n",
        "    jasper_decoder.restore_from(CHECKPOINT_DECODER, local_rank=0)\n",
        "\n",
        "    greedy_decoder = nemo_asr.GreedyCTCDecoder()\n",
        "    ctc_loss = nemo_asr.CTCLossNM(num_classes=len(labels))\n",
        "\n",
        "    # Instantiate necessary neural modules\n",
        "    data_layer = nemo_asr.AudioToTextDataLayer(\n",
        "        shuffle=False,\n",
        "        manifest_filepath=manifest,\n",
        "        labels=labels, batch_size=1)\n",
        "\n",
        "    # Define inference DAG\n",
        "    audio_signal, audio_signal_len, transcript, transcript_len = data_layer()\n",
        "\n",
        "    processed_signal, processed_signal_len = data_preprocessor(\n",
        "        input_signal=audio_signal,\n",
        "        length=audio_signal_len)\n",
        "    print(\"processed_signal\", processed_signal)\n",
        "    encoded, encoded_len = jasper_encoder(audio_signal=processed_signal,\n",
        "                                          length=processed_signal_len)\n",
        "    print(\"Encoded\", encoded)\n",
        "    log_probs = jasper_decoder(encoder_output=encoded)\n",
        "    print(\"Decoder output logprobs\",log_probs)\n",
        "    predictions = greedy_decoder(log_probs=log_probs)\n",
        "    print(\"Greedy decoder pred\", predictions)\n",
        "    loss = ctc_loss(\n",
        "            log_probs=log_probs,\n",
        "            targets=transcript,\n",
        "            input_length=encoded_len,\n",
        "            target_length=transcript_len)\n",
        "    print(\"Loss\", loss)\n",
        "    if ENABLE_NGRAM:\n",
        "        print('Running with beam search')\n",
        "        beam_predictions = beam_search_with_lm(\n",
        "            log_probs=log_probs, log_probs_length=encoded_len)\n",
        "        eval_tensors = [beam_predictions]\n",
        "\n",
        "    if greedy:\n",
        "        eval_tensors = [predictions]\n",
        "\n",
        "    tensors = neural_factory.infer(tensors=eval_tensors)\n",
        "    if greedy:\n",
        "        from nemo_asr.helpers import post_process_predictions\n",
        "        prediction = post_process_predictions(tensors[0], labels)\n",
        "    else:\n",
        "        prediction = tensors[0][0][0][0][1]\n",
        "    return (transcript,prediction)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAqlX3T9dXrU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "outputId": "e4614033-716c-4a32-abec-6732933f9524"
      },
      "source": [
        "transcript,prediction = quartzNetMultiDatasetModel_NoLM(manifestJsonPath)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-07-25 11:52:23,528 - INFO - PADDING: 16\n",
            "2020-07-25 11:52:23,528 - INFO - PADDING: 16\n",
            "2020-07-25 11:52:23,528 - INFO - PADDING: 16\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "STFT using torch\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-07-25 11:52:24,269 - INFO - Dataset loaded with 0.96 hours. Filtered 0.00 hours.\n",
            "2020-07-25 11:52:24,269 - INFO - Dataset loaded with 0.96 hours. Filtered 0.00 hours.\n",
            "2020-07-25 11:52:24,269 - INFO - Dataset loaded with 0.96 hours. Filtered 0.00 hours.\n",
            "2020-07-25 11:52:24,274 - INFO - Evaluating batch 0 out of 1106\n",
            "2020-07-25 11:52:24,274 - INFO - Evaluating batch 0 out of 1106\n",
            "2020-07-25 11:52:24,274 - INFO - Evaluating batch 0 out of 1106\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "processed_signal 0-><class 'nemo.core.neural_types.BatchTag'>:None:None\n",
            "1-><class 'nemo.core.neural_types.MelSpectrogramSignalTag'>:None:None\n",
            "2-><class 'nemo.core.neural_types.ProcessedTimeTag'>:None:None\n",
            "Encoded 0-><class 'nemo.core.neural_types.BatchTag'>:None:None\n",
            "1-><class 'nemo.core.neural_types.EncodedRepresentationTag'>:None:None\n",
            "2-><class 'nemo.core.neural_types.ProcessedTimeTag'>:None:None\n",
            "Decoder output logprobs 0-><class 'nemo.core.neural_types.BatchTag'>:None:None\n",
            "1-><class 'nemo.core.neural_types.TimeTag'>:None:None\n",
            "2-><class 'nemo.core.neural_types.ChannelTag'>:None:None\n",
            "Greedy decoder pred 0-><class 'nemo.core.neural_types.BatchTag'>:None:None\n",
            "1-><class 'nemo.core.neural_types.TimeTag'>:None:None\n",
            "Loss non-tensor object\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-07-25 11:52:30,851 - INFO - Evaluating batch 110 out of 1106\n",
            "2020-07-25 11:52:30,851 - INFO - Evaluating batch 110 out of 1106\n",
            "2020-07-25 11:52:30,851 - INFO - Evaluating batch 110 out of 1106\n",
            "2020-07-25 11:52:37,481 - INFO - Evaluating batch 220 out of 1106\n",
            "2020-07-25 11:52:37,481 - INFO - Evaluating batch 220 out of 1106\n",
            "2020-07-25 11:52:37,481 - INFO - Evaluating batch 220 out of 1106\n",
            "2020-07-25 11:52:44,252 - INFO - Evaluating batch 330 out of 1106\n",
            "2020-07-25 11:52:44,252 - INFO - Evaluating batch 330 out of 1106\n",
            "2020-07-25 11:52:44,252 - INFO - Evaluating batch 330 out of 1106\n",
            "2020-07-25 11:52:50,906 - INFO - Evaluating batch 440 out of 1106\n",
            "2020-07-25 11:52:50,906 - INFO - Evaluating batch 440 out of 1106\n",
            "2020-07-25 11:52:50,906 - INFO - Evaluating batch 440 out of 1106\n",
            "2020-07-25 11:52:57,550 - INFO - Evaluating batch 550 out of 1106\n",
            "2020-07-25 11:52:57,550 - INFO - Evaluating batch 550 out of 1106\n",
            "2020-07-25 11:52:57,550 - INFO - Evaluating batch 550 out of 1106\n",
            "2020-07-25 11:53:04,345 - INFO - Evaluating batch 660 out of 1106\n",
            "2020-07-25 11:53:04,345 - INFO - Evaluating batch 660 out of 1106\n",
            "2020-07-25 11:53:04,345 - INFO - Evaluating batch 660 out of 1106\n",
            "2020-07-25 11:53:11,678 - INFO - Evaluating batch 770 out of 1106\n",
            "2020-07-25 11:53:11,678 - INFO - Evaluating batch 770 out of 1106\n",
            "2020-07-25 11:53:11,678 - INFO - Evaluating batch 770 out of 1106\n",
            "2020-07-25 11:53:18,086 - INFO - Evaluating batch 880 out of 1106\n",
            "2020-07-25 11:53:18,086 - INFO - Evaluating batch 880 out of 1106\n",
            "2020-07-25 11:53:18,086 - INFO - Evaluating batch 880 out of 1106\n",
            "2020-07-25 11:53:24,603 - INFO - Evaluating batch 990 out of 1106\n",
            "2020-07-25 11:53:24,603 - INFO - Evaluating batch 990 out of 1106\n",
            "2020-07-25 11:53:24,603 - INFO - Evaluating batch 990 out of 1106\n",
            "2020-07-25 11:53:31,012 - INFO - Evaluating batch 1100 out of 1106\n",
            "2020-07-25 11:53:31,012 - INFO - Evaluating batch 1100 out of 1106\n",
            "2020-07-25 11:53:31,012 - INFO - Evaluating batch 1100 out of 1106\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwQ8lRbdd8sJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "2fa87b32-3ff7-4956-9a1d-23ce1182defc"
      },
      "source": [
        "prediction[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"bak god he's lost int es to alaska\",\n",
              " 'whatdid',\n",
              " 'or we could go to the bank close our accounts and cut them off at the swords',\n",
              " \"you're at ceans\",\n",
              " \"but then we won't be backbutdies\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WrAx0Hq0T2F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e226d18b-f92b-4ae3-dd9c-cbf64aef6a37"
      },
      "source": [
        "def quartzNetNemoModel_NoLM(manifest, greedy=True):\n",
        "    from ruamel.yaml import YAML\n",
        "    yaml = YAML(typ=\"safe\")\n",
        "    with open(MODEL_YAML) as f:\n",
        "        jasper_model_definition = yaml.load(f)\n",
        "    labels = jasper_model_definition['labels']\n",
        "\n",
        "    neural_factory = nemo.core.NeuralModuleFactory(\n",
        "    placement=nemo.core.DeviceType.GPU,\n",
        "    log_dir='/content/tb_logs/',\n",
        "    backend=nemo.core.Backend.PyTorch,create_tb_writer=True)\n",
        "\n",
        "    data_preprocessor = nemo_asr.AudioToMelSpectrogramPreprocessor(factory=neural_factory)\n",
        "\n",
        "    jasper_encoder = nemo_asr.JasperEncoder(\n",
        "      jasper=jasper_model_definition['JasperEncoder']['jasper'],\n",
        "      activation=jasper_model_definition['JasperEncoder']['activation'],\n",
        "      feat_in=jasper_model_definition['AudioToMelSpectrogramPreprocessor']['features'])\n",
        "\n",
        "    jasper_encoder.restore_from(CHECKPOINT_ENCODER, local_rank=0)\n",
        "\n",
        "\n",
        "    jasper_decoder = nemo_asr.JasperDecoderForCTC(\n",
        "        feat_in=1024,\n",
        "        num_classes=len(labels))\n",
        "    jasper_decoder.restore_from(CHECKPOINT_DECODER, local_rank=0)\n",
        "\n",
        "    greedy_decoder = nemo_asr.GreedyCTCDecoder()\n",
        "    ctc_loss = nemo_asr.CTCLossNM(num_classes=len(labels))\n",
        "\n",
        "    # Instantiate necessary neural modules\n",
        "    data_layer = nemo_asr.AudioToTextDataLayer(\n",
        "        shuffle=False,\n",
        "        manifest_filepath=manifest,\n",
        "        labels=labels, batch_size=1)\n",
        "\n",
        "    # Define inference DAG\n",
        "    audio_signal, audio_signal_len, transcript, transcript_len = data_layer()\n",
        "\n",
        "    processed_signal, processed_signal_len = data_preprocessor(\n",
        "        input_signal=audio_signal,\n",
        "        length=audio_signal_len)\n",
        "    print(\"processed_signal\", processed_signal)\n",
        "    encoded, encoded_len = jasper_encoder(audio_signal=processed_signal,\n",
        "                                          length=processed_signal_len)\n",
        "    print(\"Encoded\", encoded)\n",
        "    log_probs = jasper_decoder(encoder_output=encoded)\n",
        "    print(\"Decoder output logprobs\",log_probs)\n",
        "    predictions = greedy_decoder(log_probs=log_probs)\n",
        "    print(\"Greedy decoder pred\", predictions)\n",
        "    loss = ctc_loss(\n",
        "            log_probs=log_probs,\n",
        "            targets=transcript,\n",
        "            input_length=encoded_len,\n",
        "            target_length=transcript_len)\n",
        "    print(\"Loss\", loss)\n",
        "    if ENABLE_NGRAM:\n",
        "        print('Running with beam search')\n",
        "        beam_predictions = beam_search_with_lm(\n",
        "            log_probs=log_probs, log_probs_length=encoded_len)\n",
        "        eval_tensors = [beam_predictions]\n",
        "\n",
        "    if greedy:\n",
        "        eval_tensors = [predictions]\n",
        "\n",
        "    tensors = neural_factory.infer(tensors=eval_tensors)\n",
        "    if greedy:\n",
        "        from nemo_asr.helpers import post_process_predictions\n",
        "        prediction = post_process_predictions(tensors[0], labels)\n",
        "    else:\n",
        "        prediction = tensors[0][0][0][0][1]\n",
        "    return (transcript,prediction)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<nemo.core.neural_types.NmTensor at 0x7f4801eb8048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-2rYXoFPBWq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "Literature survey\n",
        "Infer on MELD dataset\n",
        "Accuracy - 60-70%\n",
        "\n",
        "# TODOs\n",
        "QuartzNet Multi-dataset without LM \n",
        "QuartzNet Multi-dataset with LM (BERT , Khaldi's LM) #https://docs.nvidia.com/deeplearning/nemo/developer_guide/en/stable/\n",
        "QuartzNet Multi-dataset without LM : .nemo tar file\n",
        "QuartzNet Nemo model without LM # https://colab.research.google.com/drive/1v_PPJQfNCkAvpc_x-eordoMzz1FaxQGd#scrollTo=jLtlNuMu0LSl\n",
        "DeepSpeech #https://colab.research.google.com/drive/1bOWBv1uivHdrQUeSPDLyzPsdvThbCXR7#scrollTo=mRTU7BdB389u\n",
        "\n",
        "For all these find the avergae WER  for the dev dataset--  get text from the transcript Neural Module\n",
        "\n",
        "!pip install jiwer\n",
        "wer = jiwer.wer(expected_output, predicted_output)\n",
        "\n",
        "# Understand what is CTC loss , Quartznet mode architecture\n",
        "As per papers what are the WER on train and test sets for all the models\n",
        "\n",
        "\n",
        "Improving using BERT LM\n",
        "Tried on our instruction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwNkfvgv0TOz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Speech to text\n",
        "# infered on  MELD training dataset \n",
        "# Use Pretrained LM \n",
        "# identify accuracy\n",
        "\n",
        "#Instruction - infer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfW3wMATd8vJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Text to image\n",
        "# GAN : Nvidia\n",
        "# Coco dataset\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6FlKe6Ed8yN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aj8Ls8oxd82k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "entagvdsd86H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqxbEe55d80b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-pELnlR1Yhh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}